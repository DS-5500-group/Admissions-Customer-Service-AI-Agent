# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

_file_map = {

    "app_functions.baml": "// initial trial classification schema for university questions\r\nclass UniQuery {\r\n    group \"Admission Requirements Domestic\" | \"Application Questions\"\r\n    subgroup \"GPA\" | \"Dates\" //@description(\"sub group related to primary group)\" // description is decorator: will help LLM\r\n        \r\n}\r\n\r\n// enum RankingSubgroup {OVERALL\r\n// CS \r\n// HEALTH_SCIENCE \r\n// MATHS \r\n// ENGINEERING \r\n// BUSINESS \r\n// ARTS_HUMANITIES \r\n// SOCIAL_SCIENCE \r\n// LAW \r\n// MEDICINE \r\n// NURSING \r\n// PHARMACY}\r\nenum FinancialSubgroup {\r\nHOUSING\r\nTUITION\r\nOTHER\r\n}\r\nenum LocationSubgroup {\r\nBOSTON\r\nOAKLAND\r\nNEW_YORK\r\n}\r\nenum CareerSubgroup {\r\nCOOP_INTERNSHIPS\r\nOUTCOMES\r\nLONG_TERM\r\n}\r\nenum ApplicationSubgroup {\r\nDATES\r\nFEES\r\nED\r\nEA\r\n}\r\nenum AdmissionDomesticSubgroup {\r\nGPA\r\nSAT\r\nACT\r\n}\r\nenum AdmissionInternationalSubgroup {\r\nGPA\r\nSAT\r\nACT\r\nLANGUAGE\r\n}\r\nenum ContactSubgroup {\r\nADMISSIONS\r\nHEALTH_COUNSELING\r\nDISABILITY\r\n}\r\n\r\nclass RankingQuestion {\r\n    Ranking_Subgroup \"Overall\" | \"CS\" | \"HealthScience\" | \"Math\" | \"Engineering\" | \"Business\"\r\n}\r\n\r\n// class RankingQuestion {\r\n//     Ranking_Subgroup RankingSubgroup\r\n// }\r\n\r\nclass FinancialQuestion {\r\n    Financial_Subgroup FinancialSubgroup\r\n}\r\n\r\nclass LocationQuestion {\r\n    Location_Subgroup LocationSubgroup\r\n}\r\n\r\nclass CareerQuestion {\r\n    Career_Subgroup CareerSubgroup\r\n}\r\n\r\nclass ApplicationQuestion {\r\n    Application_Subgroup ApplicationSubgroup\r\n}\r\n\r\nclass AdmissionDomesticQuestion {\r\n    AdmissionDomestic_Subgroup AdmissionDomesticSubgroup\r\n}\r\n\r\nclass AdmissionInternationalQuestion {\r\n    AdmissionInternational_Subgroup AdmissionInternationalSubgroup\r\n}\r\n\r\nclass ContactQuestion {\r\n    Contact_Subgroup ContactSubgroup\r\n}\r\n\r\nclass OtherQuestion {\r\n    UnAnswerable string  // Why it's unanswerable\r\n}\r\n\r\nclass ClassifiedQuestion {\r\n    category RankingQuestion | FinancialQuestion | LocationQuestion | CareerQuestion | ApplicationQuestion | AdmissionDomesticQuestion | AdmissionInternationalQuestion | ContactQuestion | OtherQuestion\r\n    //universityMentioned string?\r\n    //confidence float\r\n}\r\n\r\n\r\n// function 1: parse user question\r\n// \"{{ ctx.output_format }}\" causes JSON formatting request to be added to the prompt based on BAML class\r\n//   Can view this extra prompt in the BAML Playground (requires VScode extension)\r\nfunction ParseQuery(question:string) -> ClassifiedQuestion {\r\n    client CustomGPT\r\n    prompt #\"\r\n        Classify the input user question into the question Group and its SubGroup. \r\n\r\n        Question: {{question}}\r\n\r\n        {{ ctx.output_format }}\r\n    \"#\r\n}\r\n\r\nfunction AnswerQuery(question:string, context:string) -> string {\r\n    client CustomGPT\r\n    prompt #\"\r\n        Answer the user's question concisely and accurately using the context provided.\r\n\r\n        User question: {{question}}\r\n\r\n        University info context: {{context}}\r\n    \"#\r\n}\r\n\r\n\r\ntest Test1Parse {\r\n  functions [ParseQuery]\r\n  args {\r\n    question #\"\r\n      What GPA does Northeastern require for admissions?\r\n    \"#\r\n  }\r\n}\r\n",
    "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\r\n\r\n// Using the new OpenAI Responses API for enhanced formatting\r\nclient<llm> CustomGPT5 {\r\n  provider openai-responses\r\n  options {\r\n    model \"gpt-5\"\r\n    api_key env.OPENAI_API_KEY\r\n  }\r\n}\r\n\r\nclient<llm> CustomGPT5Mini {\r\n  provider openai-responses\r\n  retry_policy Exponential\r\n  options {\r\n    model \"gpt-5-mini\"\r\n    api_key env.OPENAI_API_KEY\r\n  }\r\n}\r\n\r\n// Openai with chat completion\r\nclient<llm> CustomGPT5Chat {\r\n  provider openai\r\n  options {\r\n    model \"gpt-5\"\r\n    api_key env.OPENAI_API_KEY\r\n  }\r\n}\r\n\r\nclient<llm> CustomGPT {\r\n  provider openai\r\n  options{\r\n    model \"gpt-4o-mini\"\r\n    api_key env.OPENAI_API_KEY\r\n  }\r\n  \r\n}\r\n\r\n// Latest Anthropic Claude 4 models\r\nclient<llm> CustomOpus4 {\r\n  provider anthropic\r\n  options {\r\n    model \"claude-opus-4-1-20250805\"\r\n    api_key env.ANTHROPIC_API_KEY\r\n  }\r\n}\r\n\r\nclient<llm> CustomSonnet4 {\r\n  provider anthropic\r\n  options {\r\n    model \"claude-sonnet-4-20250514\"\r\n    api_key env.ANTHROPIC_API_KEY\r\n  }\r\n}\r\n\r\nclient<llm> CustomHaiku {\r\n  provider anthropic\r\n  retry_policy Constant\r\n  options {\r\n    model \"claude-3-5-haiku-20241022\"\r\n    api_key env.ANTHROPIC_API_KEY\r\n  }\r\n}\r\n\r\n// Example Google AI client (uncomment to use)\r\n// client<llm> CustomGemini {\r\n//   provider google-ai\r\n//   options {\r\n//     model \"gemini-2.5-pro\"\r\n//     api_key env.GOOGLE_API_KEY\r\n//   }\r\n// }\r\n\r\n// Example AWS Bedrock client (uncomment to use)\r\n// client<llm> CustomBedrock {\r\n//   provider aws-bedrock\r\n//   options {\r\n//     model \"anthropic.claude-sonnet-4-20250514-v1:0\"\r\n//     region \"us-east-1\"\r\n//     // AWS credentials are auto-detected from env vars\r\n//   }\r\n// }\r\n\r\n// Example Azure OpenAI client (uncomment to use)\r\n// client<llm> CustomAzure {\r\n//   provider azure-openai\r\n//   options {\r\n//     model \"gpt-5\"\r\n//     api_key env.AZURE_OPENAI_API_KEY\r\n//     base_url \"https://MY_RESOURCE_NAME.openai.azure.com/openai/deployments/MY_DEPLOYMENT_ID\"\r\n//     api_version \"2024-10-01-preview\"\r\n//   }\r\n// }\r\n\r\n// Example Vertex AI client (uncomment to use)\r\n// client<llm> CustomVertex {\r\n//   provider vertex-ai\r\n//   options {\r\n//     model \"gemini-2.5-pro\"\r\n//     location \"us-central1\"\r\n//     // Uses Google Cloud Application Default Credentials\r\n//   }\r\n// }\r\n\r\n// Example Ollama client for local models (uncomment to use)\r\n// client<llm> CustomOllama {\r\n//   provider openai-generic\r\n//   options {\r\n//     base_url \"http://localhost:11434/v1\"\r\n//     model \"llama4\"\r\n//     default_role \"user\" // Most local models prefer the user role\r\n//     // No API key needed for local Ollama\r\n//   }\r\n// }\r\n\r\n// https://docs.boundaryml.com/docs/snippets/clients/round-robin\r\nclient<llm> CustomFast {\r\n  provider round-robin\r\n  options {\r\n    // This will alternate between the two clients\r\n    strategy [CustomGPT5Mini, CustomHaiku]\r\n  }\r\n}\r\n\r\n// https://docs.boundaryml.com/docs/snippets/clients/fallback\r\nclient<llm> OpenaiFallback {\r\n  provider fallback\r\n  options {\r\n    // This will try the clients in order until one succeeds\r\n    strategy [CustomGPT5Mini, CustomGPT5]\r\n  }\r\n}\r\n\r\n// https://docs.boundaryml.com/docs/snippets/clients/retry\r\nretry_policy Constant {\r\n  max_retries 3\r\n  strategy {\r\n    type constant_delay\r\n    delay_ms 200\r\n  }\r\n}\r\n\r\nretry_policy Exponential {\r\n  max_retries 2\r\n  strategy {\r\n    type exponential_backoff\r\n    delay_ms 300\r\n    multiplier 1.5\r\n    max_delay_ms 10000\r\n  }\r\n}",
    "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"go\", \"rust\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"python/pydantic\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../src\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.218.1\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode sync\n}\n",
    "resume.baml": "// Defining a data model.\r\nclass Resume {\r\n  name string\r\n  email string\r\n  experience string[]\r\n  skills string[]\r\n}\r\n\r\n// Create a function to extract the resume from a string.\r\nfunction ExtractResume(resume: string) -> Resume {\r\n  // Specify a client as provider/model-name\r\n  // You can also use custom LLM params with a custom client name from clients.baml like \"client CustomGPT5\" or \"client CustomSonnet4\"\r\n  client \"openai-responses/gpt-5-mini\" // Set OPENAI_API_KEY to use this client.\r\n  prompt #\"\r\n    Extract from this content:\r\n    {{ resume }}\r\n\r\n    {{ ctx.output_format }}\r\n  \"#\r\n}\r\n\r\n\r\n\r\n// Test the function with a sample resume. Open the VSCode playground to run this.\r\ntest vaibhav_resume {\r\n  functions [ExtractResume]\r\n  args {\r\n    resume #\"\r\n      Vaibhav Gupta\r\n      vbv@boundaryml.com\r\n\r\n      Experience:\r\n      - Founder at BoundaryML\r\n      - CV Engineer at Google\r\n      - CV Engineer at Microsoft\r\n\r\n      Skills:\r\n      - Rust\r\n      - C++\r\n    \"#\r\n  }\r\n}\r\n",
}

def get_baml_files():
    return _file_map